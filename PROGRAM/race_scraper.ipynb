{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d646aa8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/bbr34hd53gj8pszhqp_frb540000gn/T/ipykernel_48955/3706641682.py:32: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(response.text)[0]\n",
      "/var/folders/8g/bbr34hd53gj8pszhqp_frb540000gn/T/ipykernel_48955/3706641682.py:32: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(response.text)[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m         race_results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([race_results[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m race_results])\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m race_results_df\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mRaceResult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m202401010501\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m202401010502\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# year = 2020\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# race_id_list = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# race_results = RaceResult.scrape(race_id_list)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# race_results.to_pickle(f'../DATA/{year}_race_result.pkl')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 88\u001b[0m, in \u001b[0;36mRaceResult.scrape\u001b[0;34m(race_ids)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m race_results_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrace_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrace_results\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m race_results_df\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "class RaceResult:\n",
    "    @staticmethod\n",
    "    def scrape(race_ids):\n",
    "        \"\"\"\n",
    "        レース結果のスクレイピングを行うよ。\n",
    "    \n",
    "        入力input:\n",
    "        レースIDのリスト\n",
    "        race_ids : list\n",
    "    \n",
    "        出力output:\n",
    "        結果のデータフレーム\n",
    "        race_results : pd.DataFrame\n",
    "        \"\"\"\n",
    "        # 辞書型で出力を定義しておく\n",
    "        race_results = {}\n",
    "        for race_id in race_ids:\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id + \"/\"\n",
    "\n",
    "                response = requests.get(url)\n",
    "                response.encoding = \"EUC-JP\"\n",
    "\n",
    "                df = pd.read_html(response.text)[0]\n",
    "                # 半角スペースがあったら除去するよ〜\n",
    "                df = df.rename(columns=lambda x: x.replace(' ', ''))\n",
    "                # 正規表現で天気とレース情報をスクレイピングするよ〜\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                text = soup.select(\"div.data_intro p\")[0].text\n",
    "                words = re.findall(r'\\w+', text)\n",
    "                for info in words:\n",
    "                    if 'm' in info:\n",
    "                        df['コース長'] = [int(''.join(re.findall(r'\\d+', info)))] * len(df)\n",
    "                    if info in ['曇', '晴', '雨', '小雨', '小雪', '雪', ]:\n",
    "                        df['天気'] = [info] * len(df)\n",
    "                    if info in ['芝', 'ダート', '障']:\n",
    "                        df['レース場'] = [info] * len(df)\n",
    "                    if info in ['良', '稍重', '重', '不良']:\n",
    "                        df['場の状態'] = [info] * len(df)\n",
    "\n",
    "                # 今度はお馬さんidと騎手さんid、調教師idを取得するよ〜\n",
    "                horse_id_list = []\n",
    "                jockey_id_list = []\n",
    "                trainer_id_list = []\n",
    "\n",
    "                horse_link_list = soup.find('table', attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile(r'^/horse/')})\n",
    "                for horse_link in horse_link_list:\n",
    "                    horse_id = int(''.join(re.findall(r'\\d+', horse_link['href'])))\n",
    "                    horse_id_list.append(horse_id)\n",
    "\n",
    "                jockey_link_list = soup.find('table', attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile(r'^/jockey/result/recent/')})\n",
    "                for jockey_link in jockey_link_list:\n",
    "                    jockey_id = int(''.join(re.findall(r'\\d+', jockey_link['href'])))\n",
    "                    jockey_id_list.append(jockey_id)\n",
    "\n",
    "                trainer_link_list = soup.find('table', attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile(r'^/trainer/result/recent/')})\n",
    "                for trainer_link in trainer_link_list:\n",
    "                    trainer_id = int(''.join(re.findall(r'\\d+', trainer_link['href'])))\n",
    "                    trainer_id_list.append(trainer_id)\n",
    "\n",
    "                df['馬id'] = horse_id_list\n",
    "                df['騎手id'] = jockey_id_list\n",
    "                df['調教師id'] = trainer_id_list\n",
    "\n",
    "                df.index = [race_id] * len(df)\n",
    "                race_results[race_id] = df\n",
    "                \n",
    "                time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            except IndexError:\n",
    "                continue\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "            \n",
    "        return race_results_df\n",
    "# print(RaceResult.scrape(['202401010501', '202401010502']))\n",
    "\n",
    "# year = 2020\n",
    "\n",
    "# race_id_list = []\n",
    "# for place in range(1, 11, 1):\n",
    "#     for kai in range(1, 7, 1):\n",
    "#         for day in range(1, 13, 1):\n",
    "#             for r in range(1, 13, 1):\n",
    "#                 race_id = str(year) + str(place).zfill(2) + str(kai).zfill(2) + str(day).zfill(2) + str(r).zfill(2)\n",
    "#                 race_id_list.append(race_id)\n",
    "\n",
    "# race_results = RaceResult.scrape(race_id_list)\n",
    "# race_results.to_pickle(f'../DATA/{year}_race_result.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6955a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/bbr34hd53gj8pszhqp_frb540000gn/T/ipykernel_48955/4251663527.py:12: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(response.text)[0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     date \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mstrptime(date_str, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY年\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm月\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m日\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mscrape_race_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m202401010501\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m, in \u001b[0;36mscrape_race_data\u001b[0;34m(race_id)\u001b[0m\n\u001b[1;32m     10\u001b[0m   response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     11\u001b[0m   response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEUC-JP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m   df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 正規表現で天気とレース情報をスクレイピングするよ〜\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1225\u001b[0m     [\n\u001b[1;32m   1226\u001b[0m         is_file_like(io),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     ]\n\u001b[1;32m   1231\u001b[0m ):\n\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/html.py:1008\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43m_data_to_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;66;03m# Cast MultiIndex header to an Index of tuples when extracting header\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;66;03m# links and replace nan with None (therefore can't use mi.to_flat_index()).\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;66;03m# This maintains consistency of selection (e.g. df.columns.str[1])\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m extract_links \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1013\u001b[0m             df\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex\n\u001b[1;32m   1014\u001b[0m         ):\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/html.py:875\u001b[0m, in \u001b[0;36m_data_to_frame\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# fill out elements of body that are \"ragged\"\u001b[39;00m\n\u001b[1;32m    874\u001b[0m _expand_elements(body)\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mTextParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tp:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:2053\u001b[0m, in \u001b[0;36mTextParser\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;124;03mConverts lists of lists/tuples into DataFrames with proper type inference\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;124;03mand optional (e.g. string to datetime) conversion. Also enables iterating\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;124;03m    `round_trip` for the round-trip converter.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2052\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:133\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    128\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[1;32m    129\u001b[0m (\n\u001b[1;32m    130\u001b[0m     columns,\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[0;32m--> 133\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[1;32m    138\u001b[0m (\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/portfolio/python_API/machine_keiba/venv/lib/python3.10/site-packages/pandas/io/parsers/python_parser.py:543\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m             columns \u001b[38;5;241m=\u001b[39m [names]\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m         columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_usecols(\n\u001b[0;32m--> 543\u001b[0m             columns, \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, num_original_columns\n\u001b[1;32m    544\u001b[0m         )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_header_line)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def scrape_race_data(race_id):\n",
    "    url = \"https://db.netkeiba.com/race/\" + race_id + \"/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.encoding = \"EUC-JP\"\n",
    "    df = pd.read_html(response.text)[0]\n",
    "  # 正規表現で天気とレース情報をスクレイピングするよ〜\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = soup.select(\"div.data_intro h1\")[0].text\n",
    "#     words = re.findall(r'\\S+', text)\n",
    "    sub_text = soup.select('p.smalltxt')[0].text\n",
    "    date_str = re.findall(r'\\S+', sub_text)[0]\n",
    "    date = dt.strptime(date_str, '%Y年%m月%d日')\n",
    "    return df\n",
    "\n",
    "print(scrape_race_data('202401010501'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a30dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16607e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b9dd228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 15546/15546 [6:23:03<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# このdfは、engineering後のデータフレームだよ。順番前後しちゃってるよ。ごめんね。\n",
    "df = pd.read_pickle('../DATA/recent_5_race_df_for_learning.pkl')\n",
    "# 既存のdfに日付データを追加するよ〜\n",
    "date_data = {}\n",
    "for race_id, each_df in tqdm(df.groupby(df.index)):\n",
    "    url = \"https://db.netkeiba.com/race/\" + race_id + \"/\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'EUC-JP'\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content = soup.select('p.smalltxt')[0].text\n",
    "    pattern = '\\d+年\\d+月\\d+日'\n",
    "    str_date = re.search(pattern, content).group()\n",
    "    race_date = dt.strptime(str_date, '%Y年%m月%d日')\n",
    "    date_data[race_id] = pd.DataFrame(([race_date] * len(each_df)), index=([race_id] * len(each_df)))\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "date_df = pd.concat([date_data[key] for key in date_data])\n",
    "df_with_date = pd.concat([df, date_df], axis=1)\n",
    "df_with_date = df_with_date.rename(columns={0: '開催年月日'})\n",
    "df_with_date.to_pickle('../DATA/df_for_learning_with_date.pkl')\n",
    "# date_data_df = pd.DataFrame(date_data)\n",
    "# df['開催年月日'] = date_data_df\n",
    "# print(df['開催年月日'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d744da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/bbr34hd53gj8pszhqp_frb540000gn/T/ipykernel_65213/3553090606.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_for_db['レースid'] = race_ids\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "half_year_ago = dt.today() + relativedelta(months=-6)\n",
    "df = pd.read_pickle('../DATA/df_for_learning_with_date.pkl')\n",
    "df_for_db = df.loc[df['開催年月日'] >= half_year_ago]\n",
    "race_ids = []\n",
    "for index, _ in df_for_db.iterrows():\n",
    "    race_ids.append(index)\n",
    "df_for_db['レースid'] = race_ids\n",
    "df_for_db = df_for_db.rename(columns={\n",
    "    '開催年月日': 'race_date',\n",
    "    'レースid': 'race_id',\n",
    "    '着順': 'order',\n",
    "    '馬名': 'horse_name',\n",
    "    '馬id': 'horse_id',\n",
    "    '騎手': 'jockey_name',\n",
    "    '騎手id': 'jockey_id',\n",
    "    '単勝': 'odds'\n",
    "})\n",
    "df_for_db = df_for_db[['race_date', 'race_id', 'order', 'horse_name', 'horse_id', 'jockey_name', 'jockey_id', 'odds']]\n",
    "\n",
    "df_for_db.to_pickle('../DATA/df_for_db_20240721.pkl')\n",
    "# for date, grouped_df in df_for_db.groupby(['race_date', 'race_id']):\n",
    "    \n",
    "# df_sorted = df_for_db.sort_values('開催年月日', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef257807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    枠  馬番   印         馬名  性齢    斤量    騎手     厩舎   馬体重(増減) Unnamed:9_level_0  \\\n",
      "    枠  馬番   印         馬名  性齢    斤量    騎手     厩舎   馬体重(増減) Unnamed:9_level_1   \n",
      "0   1   1 NaN   エナジーポケット  牡3  57.0   佐々木   栗東森田   418(-6)             ---.-   \n",
      "1   2   2 NaN     キーシンガー  牝3  52.0    長浜   美浦青木   442(+6)             ---.-   \n",
      "2   3   3 NaN  ツキガキレイデスネ  牝3  55.0     黛   美浦小島   462(+6)             ---.-   \n",
      "3   3   4 NaN    レジーナチェリ  牡3  57.0    丹内  美浦伊藤伸    434(0)             ---.-   \n",
      "4   4   5 NaN  ブライティアダイヤ  牝3  55.0    永野   美浦稲垣   464(+6)             ---.-   \n",
      "5   4   6 NaN    スリータイガー  牝3  55.0   西村淳  栗東高橋忠   428(-2)             ---.-   \n",
      "6   5   7 NaN  スウィートリワード  牝3  55.0    武豊   栗東宮本   454(-2)             ---.-   \n",
      "7   5   8 NaN    オメガサミット  牡3  56.0   角田河   栗東今野  450(-16)             ---.-   \n",
      "8   6   9 NaN   インマイポケット  牝3  55.0  ルメール   栗東武幸   428(-2)             ---.-   \n",
      "9   6  10 NaN        スコア  牡3  54.0    高杉   美浦栗田   458(+4)             ---.-   \n",
      "10  7  11 NaN  シャーリーゴールド  牝3  55.0    浜中   栗東須貝    444(0)             ---.-   \n",
      "11  7  12 NaN    ローマンレイク  牝3  55.0   鮫島駿  栗東松永幹  432(+18)             ---.-   \n",
      "12  8  13 NaN   ヴァイタルピース  セ3  57.0    菱田  栗東鈴木孝   452(-4)             ---.-   \n",
      "13  8  14 NaN  ココシャンパーニュ  牝3  55.0   北村友   栗東音無  454(-10)             ---.-   \n",
      "\n",
      "    人気 お気に入り馬        horse_id jockey_id  \n",
      "    人気     登録  メモ                        \n",
      "0   **    NaN NaN  2021103298     01197  \n",
      "1   **    NaN NaN  2021102566     01214  \n",
      "2   **    NaN NaN  2021103328     01109  \n",
      "3   **    NaN NaN  2021106790     01091  \n",
      "4   **    NaN NaN  2021102468     01188  \n",
      "5   **    NaN NaN  2021110074     01171  \n",
      "6   **    NaN NaN  2021100415     00666  \n",
      "7   **    NaN NaN  2021105218     01199  \n",
      "8   **    NaN NaN  2021105466     05339  \n",
      "9   **    NaN NaN  2021100453     01213  \n",
      "10  **    NaN NaN  2021104918     01115  \n",
      "11  **    NaN NaN  2021105976     01157  \n",
      "12  **    NaN NaN  2021110079     01144  \n",
      "13  **    NaN NaN  2021105902     01102  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class NewRace:\n",
    "    @staticmethod\n",
    "    def scrape_new_race(new_race_id):\n",
    "        url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + new_race_id\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.encoding = 'EUC-JP'\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            main_text = soup.select('div.RaceList_Item02 h1')[0].text\n",
    "            if '新馬' in main_text:\n",
    "                return\n",
    "            elif '未出走' in main_text:\n",
    "                return\n",
    "\n",
    "            df = pd.read_html(response.text)[0]\n",
    "            df = df.rename(columns=lambda x: x.replace(' ', ''))\n",
    "\n",
    "            horse_id_list = []\n",
    "            jockey_id_list = []\n",
    "            \n",
    "            horse_link_list = soup.find('table', attrs={'class': 'Shutuba_Table'}).find_all('a', attrs={'href': re.compile(r'^https://db.netkeiba.com/horse/\\d+')})\n",
    "            for horse_link in horse_link_list:\n",
    "                horse_id = ''.join(re.findall(r'\\d+', horse_link['href']))\n",
    "                horse_id_list.append(horse_id)\n",
    "                \n",
    "            jockey_link_list = soup.find('table', attrs={'class': 'Shutuba_Table'}).find_all('a', attrs={'href': re.compile(r'^https://db.netkeiba.com/jockey/result/recent/\\d+')})\n",
    "            for jockey_link in jockey_link_list:\n",
    "                jockey_id = ''.join(re.findall(r'\\d+', jockey_link['href']))\n",
    "                jockey_id_list.append(jockey_id)\n",
    "                \n",
    "            df['horse_id'] = horse_id_list\n",
    "            df['jockey_id'] = jockey_id_list\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return df\n",
    "\n",
    "print(NewRace.scrape_new_race('202401010401'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a057fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['202404020301', '202404020302', '202404020303', '202404020304', '202404020305', '202404020306', '202404020307', '202404020308', '202404020309', '202404020310', '202404020311', '202404020312', '202401010501', '202401010502', '202401010503', '202401010504', '202401010505', '202401010506', '202401010507', '202401010508', '202401010509', '202401010510', '202401010511', '202401010512']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--hide-scrollbars')\n",
    "options.add_argument('--no-sandbox')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def get_new_race_id(date: str) -> list:\n",
    "    url = 'https://race.netkeiba.com/top/?kaisai_date=' + date\n",
    "    response = driver.get(url)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    race_id_list = []\n",
    "\n",
    "    link_list = soup.find('div', attrs={'class': 'RaceList_Box'}).find_all('a', attrs={'href': re.compile(r'^../race/(shutuba|result)\\.html')})\n",
    "    for link in link_list:\n",
    "        race_id = ''.join(re.findall(r'\\d+', link['href']))\n",
    "        race_id_list.append(race_id)\n",
    "    return race_id_list\n",
    "\n",
    "print(get_new_race_id('20240803'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7954949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240803\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "today = dt.today()\n",
    "str_today = today.strftime('%Y%m%d')\n",
    "print(str_today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee9f585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '5', '3', '7', '11', '2', '1', '8']\n"
     ]
    }
   ],
   "source": [
    "list_a = [1,2,3,4,5,2,3,5,7,11,1,1,2,3,5,8]\n",
    "list_b = []\n",
    "for value in list_a:\n",
    "    list_b.append(str(value))\n",
    "print(list(set(list_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ea1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ea02be749820b6e9834dacc98cdba2a78d68624d8dbb276cbdaece6e40651d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
